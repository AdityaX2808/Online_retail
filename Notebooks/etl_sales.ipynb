{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51844824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, current_timestamp\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ETL Sales Fact\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.6.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "jdbc_url = \"jdbc:postgresql://localhost:5432/retail_dw\"\n",
    "conn_props = {\"user\": \"your_user\", \"password\": \"your_password\", \"driver\": \"org.postgresql.Driver\"}\n",
    "\n",
    "# Read sales staging data\n",
    "sales_stage = spark.read.jdbc(jdbc_url, \"wh_stage.stg_sales_fact\", properties=conn_props)\n",
    "\n",
    "# Cast for consistency\n",
    "sales_stage = sales_stage.withColumn(\"quantity\", col(\"quantity\").cast(IntegerType())) \\\n",
    "                         .withColumn(\"unit_price\", col(\"unit_price\").cast(DoubleType()))\n",
    "\n",
    "# In production, join with core.dim_customer and core.dim_product to get surrogate keys\n",
    "sales_stage = sales_stage.withColumn(\"etl_insert_ts\", current_timestamp()) \\\n",
    "                         .withColumn(\"etl_update_ts\", current_timestamp())\n",
    "\n",
    "sales_stage.write.jdbc(jdbc_url, \"core.fact_sales\", mode=\"append\", properties=conn_props)\n",
    "\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
